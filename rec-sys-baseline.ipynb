{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d703e89",
   "metadata": {},
   "source": [
    "# Recommender Systems Baseline\n",
    "\n",
    "Notebook covers the user-based KNN and SVD recommender systems to compare against the Social Media Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6b12fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD, AlgoBase, KNNBaseline\n",
    "from surprise.model_selection import cross_validate, train_test_split, LeaveOneOut\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed9989",
   "metadata": {},
   "source": [
    "## Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e0ffd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>SwiftGGTeam/the-swift-programming-language-in-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>atom/atom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>capistrano/capistrano</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>git/git</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>golang/go</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id                                            repo_id  rating\n",
       "0  0x00evil  SwiftGGTeam/the-swift-programming-language-in-...       2\n",
       "1  0x00evil                                          atom/atom       3\n",
       "2  0x00evil                              capistrano/capistrano       3\n",
       "3  0x00evil                                            git/git       3\n",
       "4  0x00evil                                          golang/go       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/user-items-ratings-improved.csv', \n",
    "                 names=['user_id', 'repo_id', 'rating'], \n",
    "                 skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca96c847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>atom/atom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>capistrano/capistrano</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>git/git</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>golang/go</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>rails/rails</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>torvalds/linux</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                repo_id  rating\n",
       "1   0x00evil              atom/atom       3\n",
       "2   0x00evil  capistrano/capistrano       3\n",
       "3   0x00evil                git/git       3\n",
       "4   0x00evil              golang/go       3\n",
       "6   0x00evil            rails/rails       1\n",
       "10  0x00evil         torvalds/linux       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['user_id'] == '0x00evil') & df['rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f12676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total obs: 9184\n",
      "Total users: 1162\n",
      "Total repos: 272\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_repos = df.repo_id.unique().shape[0]\n",
    "\n",
    "print(f'Total obs: {df.shape[0]}')\n",
    "print(f'Total users: {n_users}')\n",
    "print(f'Total repos: {n_repos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03307949",
   "metadata": {},
   "source": [
    "## Prep data for Rec Sys\n",
    "\n",
    "Data has already been loaded in as a sparse matrix, we collect the per repo rankings, that is those repositories with most stars in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7283b0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo_id\n",
       "twbs/bootstrap                            340\n",
       "EbookFoundation/free-programming-books    296\n",
       "nodejs/node-v0.x-archive                  224\n",
       "d3/d3                                     221\n",
       "sindresorhus/awesome                      204\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_rankings = df.groupby(['repo_id']).rating.sum().sort_values(ascending=False)\n",
    "repo_rankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ba53ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>0</td>\n",
       "      <td>SwiftGGTeam/the-swift-programming-language-in-...</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>0</td>\n",
       "      <td>atom/atom</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>0</td>\n",
       "      <td>capistrano/capistrano</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>0</td>\n",
       "      <td>git/git</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x00evil</td>\n",
       "      <td>0</td>\n",
       "      <td>golang/go</td>\n",
       "      <td>115</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_name  user_id                                          repo_name  \\\n",
       "0  0x00evil        0  SwiftGGTeam/the-swift-programming-language-in-...   \n",
       "1  0x00evil        0                                          atom/atom   \n",
       "2  0x00evil        0                              capistrano/capistrano   \n",
       "3  0x00evil        0                                            git/git   \n",
       "4  0x00evil        0                                          golang/go   \n",
       "\n",
       "   repo_id  rating  \n",
       "0       28       2  \n",
       "1       53       3  \n",
       "2       64       3  \n",
       "3      110       3  \n",
       "4      115       3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alter the ratings? - this has to be done to compute cosine sims, the distance remains the same so it shouldnt be a problem\n",
    "temp = df.copy()\n",
    "# temp['rating'] = temp['rating']+1\n",
    "\n",
    "temp['user_id'] = temp['user_id'].astype('category')\n",
    "temp['repo_id'] = temp['repo_id'].astype('category')\n",
    "\n",
    "# ulabels, ulevels = pd.factorize(temp['user_id'])\n",
    "# rlabels, rlevels = pd.factorize(temp['repo_id'])\n",
    "temp = pd.DataFrame({\n",
    "    'user_name': temp['user_id'],\n",
    "    'user_id': temp['user_id'].cat.codes,\n",
    "    'repo_name': temp['repo_id'],\n",
    "    'repo_id': temp['repo_id'].cat.codes,\n",
    "    'rating': temp['rating']\n",
    "})\n",
    "\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae892a36",
   "metadata": {},
   "source": [
    "# CUSTOM RECOMMENDER\n",
    "\n",
    "Open triads recommender here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a2acaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FollowingOnlyAlgorithm(AlgoBase):\n",
    "    \"\"\"This rec algo takes in the repositories that a followed users have starred at gives a 2.0 if present else 1.0\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "        \n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"Update to previous, now we only calc those users we know\"\"\"\n",
    "        if self.trainset.knows_user(u) and self.trainset.knows_item(i):\n",
    "            raw_uid = self.trainset.to_raw_uid(u)\n",
    "            raw_iid = self.trainset.to_raw_iid(i)\n",
    "            \n",
    "            # Extract the original user name and repo name\n",
    "            uname = list(temp[temp['user_id'] == raw_uid].user_name)[0]\n",
    "            rname = list(temp[temp['repo_id'] == raw_iid].repo_name)[0]\n",
    "            following_repos = self.get_following_set(uname)\n",
    "            \n",
    "            # If the user is following someone that stars the current repo (raw_iid) then return 1, else 0\n",
    "            if rname in following_repos:\n",
    "                return np.clip(following_repos[uname], 1, 3)\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        return self.trainset.global_mean\n",
    "\n",
    "    \n",
    "    def get_following_set(self, uname):\n",
    "        \"\"\"Finds the egonet of following for a given user and returns the set of those\"\"\"\n",
    "        curr_following = None\n",
    "        try:\n",
    "            \n",
    "            with open('./data/egonets/following/'+uname+'.json') as fp:\n",
    "                curr_following = json.load(fp)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Error: unable to find file for {raw_uid}.')\n",
    "            return set()\n",
    "        except Exception as e:\n",
    "            print(f'ERROR: failed on raw uid: {raw_uid}')\n",
    "            print(e)\n",
    "            return set()\n",
    "\n",
    "\n",
    "        following_repos = []\n",
    "        for f in curr_following['nodes']:\n",
    "            repos = f['starredRepositories']['nodes']\n",
    "            following_repos += repos\n",
    "\n",
    "        following_repos = Counter([repo['nameWithOwner'] for repo in following_repos])\n",
    "        return following_repos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932d1f9",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "> Note the below code was derived and adapted from COSC2933.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbe2a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from surprise import accuracy\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class EData:\n",
    "    def __init__(self, data, rankings):\n",
    "        # Generate entire training set for evaluating properties \n",
    "        self.full_trainset = data.build_full_trainset()\n",
    "        self.full_anti_testset = self.full_trainset.build_anti_testset()\n",
    "        \n",
    "        # Set rankings\n",
    "        self.rankings = rankings\n",
    "        \n",
    "        # Generate a train (75) / test (25) split for measuring accuracy\n",
    "        self.trainset, self.testset = train_test_split(data, test_size=0.25, random_state=1)\n",
    "        \n",
    "        # Create leave-one-out train/test split for eval of top-N recs\n",
    "        # and we create an anti-test-set for generating predictions\n",
    "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
    "        for train, test in LOOCV.split(data):\n",
    "            self.LOOCVTrain = train\n",
    "            self.LOOCVTest = test\n",
    "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
    "        \n",
    "        # Now compute sim matrix between users to measure diversity\n",
    "        self.simAlgo = KNNBaseline(\n",
    "            sim_options={'name': 'cosine', 'user_based': True}\n",
    "        )\n",
    "        self.simAlgo.fit(self.full_trainset)\n",
    "        \n",
    "        \n",
    "    def get_full_trainset(self):\n",
    "        return self.full_trainset\n",
    "    \n",
    "    def get_full_anti_testset(self):\n",
    "        return self.full_anti_testset\n",
    "    \n",
    "    def get_trainset(self):\n",
    "        return self.trainset\n",
    "    \n",
    "    def get_testset(self):\n",
    "        return self.testset\n",
    "    \n",
    "    def get_LOOCV_testset(self):\n",
    "        return self.LOOCVTest\n",
    "    \n",
    "    def get_LOOCV_trainset(self):\n",
    "        return self.LOOCVTrain\n",
    "    \n",
    "    def get_LOOCV_anti_testset(self):\n",
    "        return self.LOOCVAntiTestSet\n",
    "    \n",
    "    def get_rankings(self):\n",
    "        return self.rankings\n",
    "    \n",
    "    def get_sims(self):\n",
    "        return self.simAlgo\n",
    "\n",
    "class Algorithm:\n",
    "    \n",
    "    def __init__(self, name, algorithm):\n",
    "        self.name = name\n",
    "        self.algo = algorithm\n",
    "        \n",
    "    def evaluate(self, eval_data):\n",
    "        metrics = {}\n",
    "        \n",
    "        # Compute accuracy\n",
    "        print(\"Evaluating accuracy...\")\n",
    "        self.algo.fit(eval_data.get_trainset())\n",
    "        preds = self.algo.test(eval_data.get_testset())\n",
    "        metrics['RMSE'] = Metrics.RMSE(preds)\n",
    "        metrics['MAE'] = Metrics.MAE(preds)\n",
    "        \n",
    "        # Eval top-10 via leave-one-out\n",
    "        print(\"Evaluating top-10 with LOOCV..\")\n",
    "        self.algo.fit(eval_data.get_LOOCV_trainset())\n",
    "        lo_preds = self.algo.test(eval_data.get_LOOCV_testset())\n",
    "            \n",
    "        # Generate preds for ratings not in training\n",
    "        all_preds = self.algo.test(eval_data.get_LOOCV_anti_testset())\n",
    "      \n",
    "        # Get top 10 recs per user\n",
    "        topN_preds = Metrics.get_topN(all_preds, 10)\n",
    "        \n",
    "        print('Evaluating rank metrics...')\n",
    "        # Hit-rate - how often a repo that the user liked was recommended\n",
    "        metrics['HR'] = Metrics.hit_rate(topN_preds, lo_preds)\n",
    "        \n",
    "        # Cumulative-hit-rate\n",
    "        metrics['CHR'] = Metrics.cumulative_hit_rate(topN_preds, lo_preds)\n",
    "\n",
    "        metrics[\"ARHR\"] = Metrics.avg_reciprocal_hit_rate(topN_preds, lo_preds)\n",
    "\n",
    "        print('Computing recs with complete dataset...')\n",
    "        self.algo.fit(eval_data.get_full_trainset())\n",
    "        all_preds = self.algo.test(eval_data.get_full_anti_testset())\n",
    "        topN_preds = Metrics.get_topN(all_preds, 10)\n",
    "        \n",
    "        metrics['Coverage'] = Metrics.user_coverage(topN_preds, eval_data.get_full_trainset().n_users)\n",
    "        metrics['Diversity'] = Metrics.diversity(topN_preds, eval_data.get_sims())\n",
    "        metrics['Novelty'] = Metrics.novelty(topN_preds, eval_data.get_rankings())\n",
    "        \n",
    "        print('Done.')\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "        \n",
    "class Evaluator:\n",
    "    algorithms = []\n",
    "    \n",
    "    def __init__(self, data, rankings):\n",
    "        self.data = EData(data, rankings)\n",
    "    \n",
    "    def add_algorithm(self, name, algorithm):\n",
    "        self.algorithms.append(Algorithm(name, algorithm))\n",
    "    \n",
    "    def evaluate(self):\n",
    "        results = {}\n",
    "        for algo in self.algorithms:\n",
    "            print(f'Evaluating {algo.name}...')\n",
    "            results[algo.name] = algo.evaluate(self.data)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"CHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\" ))\n",
    "\n",
    "        for name, metrics in results.items():\n",
    "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"CHR\"],  metrics[\"ARHR\"], metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
    "     \n",
    "    \n",
    "    \n",
    "class Metrics:\n",
    "    \n",
    "    def MAE(predictions):\n",
    "        return accuracy.mae(predictions, verbose=False)\n",
    "    \n",
    "    def RMSE(predictions):\n",
    "        return accuracy.rmse(predictions, verbose=False)\n",
    "    \n",
    "    def get_topN(predictions, n=10):\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, actual, estimated, _ in predictions:\n",
    "            if (estimated >= 1.0): # >1 is min rating\n",
    "                top_n[int(uid)].append((int(iid), estimated))\n",
    "                \n",
    "        for uid, ratings in top_n.items():\n",
    "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[int(uid)] = ratings[:n]\n",
    "        \n",
    "        return top_n\n",
    "        \n",
    "    def hit_rate(top_n_preds, left_out_preds):\n",
    "        \"\"\"Determine the hit-rate (how good) of top-N list\"\"\"\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        for lo_pred in left_out_preds:\n",
    "            lo_uid = lo_pred[0]\n",
    "            lo_iid = lo_pred[1]\n",
    "            \n",
    "            # Check if in top 10\n",
    "            is_hit = False\n",
    "            for tn_iid, tn_pred in top_n_preds[int(lo_uid)]:\n",
    "                if (int(lo_iid) == int(tn_iid)):\n",
    "                    is_hit = True\n",
    "                    break\n",
    "            if is_hit:\n",
    "                hits += 1\n",
    "            total += 1\n",
    "            \n",
    "        precision = hits / total\n",
    "        return precision\n",
    "            \n",
    "    \n",
    "    def cumulative_hit_rate(topN_preds, left_out_preds):\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        for lo_uid, lo_iid, actual, estimated, _ in left_out_preds:\n",
    "            if (actual >= 1.0): # Only look at things the user starred\n",
    "                is_hit = False\n",
    "                for tn_iid, tn_pred in topN_preds[int(lo_uid)]:\n",
    "                    if (int(lo_iid) == tn_iid):\n",
    "                        is_hit = True\n",
    "                        break\n",
    "                if is_hit:\n",
    "                    hits += 1\n",
    "                total += 1\n",
    "        precision = hits/total\n",
    "        return precision\n",
    "    \n",
    "    def avg_reciprocal_hit_rate(topN_preds, left_out_preds):\n",
    "        S = 0\n",
    "        total = 0\n",
    "        for lo_uid, lo_iid, actual, estimated, _ in left_out_preds:\n",
    "            hit_rank = 0\n",
    "            rank = 0\n",
    "            for tn_iid, tn_pred in topN_preds[int(lo_uid)]:\n",
    "                rank += 1\n",
    "                if (int(lo_iid) == tn_iid):\n",
    "                    hit_rank = rank\n",
    "                    break\n",
    "            if hit_rank > 0:\n",
    "                S += 1.0 / hit_rank\n",
    "            total += 1\n",
    "        return S / total\n",
    "    \n",
    "    def user_coverage(topN_preds, num_users):\n",
    "        # Calc the percentage of users that have at least 1 good rec\n",
    "        hits = 0\n",
    "        for tn_uid in topN_preds.keys():\n",
    "            is_hit = False\n",
    "            for tn_iid, tn_pred in topN_preds[tn_uid]:\n",
    "                if tn_pred >= 2.0: # Todo is 1 correct number to use??\n",
    "                    is_hit = True\n",
    "                    break\n",
    "            if is_hit:\n",
    "                hits += 1\n",
    "        return hits / num_users\n",
    "    \n",
    "    \n",
    "    # TODO: NOVELTY AND DIVERSITY ARE NOT WORKING -- most likely due to topN_preds not working??\n",
    "    def diversity(topN_preds, sim_algorithm):\n",
    "        n = 0\n",
    "        total = 0\n",
    "        mat = sim_algorithm.compute_similarities()\n",
    "        for uid in topN_preds.keys():\n",
    "            pairs = itertools.combinations(topN_preds[uid], 2)\n",
    "            for pair in pairs:\n",
    "                repo1 = pair[0][0]\n",
    "                repo2 = pair[1][0]\n",
    "                if sim_algorithm.trainset.knows_item(repo1) and sim_algorithm.trainset.knows_item(repo2):\n",
    "                    inner_id1 = sim_algorithm.trainset.to_inner_iid(repo1) # used to be str(repo1)\n",
    "                    inner_id2 = sim_algorithm.trainset.to_inner_iid(repo2)\n",
    "                sim = mat[inner_id1][inner_id2]\n",
    "                total += sim\n",
    "                n += 1\n",
    "        return 1 - (total / n) \n",
    "    \n",
    "    def novelty(topN_preds, rankings):\n",
    "        n = 0\n",
    "        total = 0\n",
    "        for uid in topN_preds.keys():\n",
    "            for rating in topN_preds[uid]:\n",
    "                iid = rating[0]\n",
    "                rank = rankings[iid]\n",
    "                total += rank\n",
    "                n += 1\n",
    "        return total / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84e019d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.Algorithm at 0x124face20>,\n",
       " <__main__.Algorithm at 0x120dc9ca0>,\n",
       " <__main__.Algorithm at 0x120dc9370>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process our data\n",
    "reader = Reader(rating_scale=(1, 3))\n",
    "data = Dataset.load_from_df(temp[['user_id', 'repo_id', 'rating']], reader)\n",
    "\n",
    "# setup an evaluator\n",
    "evaluator = Evaluator(data, temp.groupby(['repo_id']).rating.sum().sort_values(ascending=False))\n",
    "\n",
    "evaluator.algorithms = []\n",
    "\n",
    "# USER-BASED KNN\n",
    "user_knn = KNNBasic(sim_options={'name': 'cosine', 'user_based': True})\n",
    "\n",
    "# SVD\n",
    "svd = SVD()\n",
    "\n",
    "# Add the algorithms\n",
    "evaluator.add_algorithm('User KNN', user_knn)\n",
    "evaluator.add_algorithm('SVD', svd)\n",
    "\n",
    "# Following Only\n",
    "evaluator.add_algorithm('Following Only', FollowingOnlyAlgorithm())\n",
    "\n",
    "evaluator.algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "270ec821",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating User KNN...\n",
      "Evaluating accuracy...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating top-10 with LOOCV..\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating rank metrics...\n",
      "Computing recs with complete dataset...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Done.\n",
      "Evaluating SVD...\n",
      "Evaluating accuracy...\n",
      "Evaluating top-10 with LOOCV..\n",
      "Evaluating rank metrics...\n",
      "Computing recs with complete dataset...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Done.\n",
      "Evaluating Following Only...\n",
      "Evaluating accuracy...\n",
      "Evaluating top-10 with LOOCV..\n",
      "Evaluating rank metrics...\n",
      "Computing recs with complete dataset...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Done.\n",
      "\n",
      "Algorithm  RMSE       MAE        HR         CHR        ARHR       Coverage   Diversity  Novelty   \n",
      "User KNN   0.8876     0.7463     0.0155     0.0155     0.0054     1.0000     0.8398     52.4514   \n",
      "SVD        0.8451     0.7270     0.0284     0.0284     0.0107     0.9991     0.8334     66.9940   \n",
      "Following Only 1.2727     0.9761     0.0448     0.0448     0.0111     0.0000     0.7302     88.4052   \n"
     ]
    }
   ],
   "source": [
    "# Run the evals\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966dfb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
